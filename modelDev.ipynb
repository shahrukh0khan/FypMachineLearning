{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":["## Importing Libraries"],"metadata":{}},{"cell_type":"code","source":["\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from dask import dataframe as dd\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import gc\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge\n","from sklearn.linear_model import Lasso\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n","from sklearn.model_selection import cross_val_score\n","import xgboost as xgb\n"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Loading Data through Dask "],"metadata":{}},{"cell_type":"code","source":["data = dd.read_csv('../input/09-project3/09_Project3.csv')\n","loc_data = dd.read_csv('../input/03-locationmaster/03_LocationMaster.csv')\n","prod_data = dd.read_csv('../input/02-productmaster/02_ProductMaster.csv')\n","date_data = dd.read_csv('../input/01-calendarmaster/01_CalendarMaster.csv')"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Merging Dataset based on Unique Key Variables"],"metadata":{}},{"cell_type":"code","source":["main_df = dd.merge(data, date_data, left_on='Date', right_on='DateKey', how='left')"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main_df = dd.merge(main_df, prod_data, left_on='ProductKey', right_on='ProductKey', how='left')"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main_df = dd.merge(main_df, loc_data, left_on='LocationKey', right_on='LocationKey', how='left')"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main_df = main_df.drop([\"DateKey\", \"CatEdition\", \"Supplier\", \"DIorDOM\", \n","                         \"Region\", \"UpstreamLocKey\", \"StockPolicy\", \"ShopFormat\"], axis=1)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Calculating Propensities "],"metadata":{}},{"cell_type":"code","source":["main_df[\"SimpleOrderPropensity\"] = (main_df.OrderedUnits / main_df.DemandUnits)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main_df[\"CollectOrderPropensity\"] = (main_df.CollectedUnits / main_df.DemandUnits)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main_df = main_df.compute() ## Compute function applies all the orevious transformations in parallel so takes up less space"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["del data, loc_data, prod_data, date_data\n","gc.collect()"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Removing Outliers "],"metadata":{}},{"cell_type":"code","source":["outlier = main_df['DemandUnits'].mean() + (3*main_df['DemandUnits'].std())\n","main_df = main_df.drop((main_df[main_df['DemandUnits'] >= outlier]).index, 0)\n","\n","outlier = main_df['OrderedUnits'].mean() + (3*main_df['OrderedUnits'].std())\n","main_df = main_df.drop((main_df[main_df['OrderedUnits'] >= outlier]).index, 0)\n","\n","outlier = main_df['CollectedUnits'].mean() + (3*main_df['CollectedUnits'].std())\n","main_df = main_df.drop((main_df[main_df['CollectedUnits'] >= outlier]).index, 0)\n","\n","main_df = main_df.drop((main_df[main_df['SimpleOrderPropensity'] > 1]).index, 0)\n","main_df = main_df.drop((main_df[main_df['CollectOrderPropensity'] > 1]).index, 0)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from sklearn.base import BaseEstimator, TransformerMixin\n","\n","# class AttrTransf(BaseEstimator, TransformerMixin):\n","    \n","#     def __init__(self, removExtraCols = True, allCatStr = True, mergCols = True):\n","#         self.removExtraCols = removExtraCols\n","#         self.allCatStr = allCatStr\n","#         self.mergCols = mergCols\n","        \n","#     def fit(self, X, y=None):\n","#         return self\n","        \n","#     def transform(self, X):\n","        \n","#     ### calculating outlier for DemandUnits (mean into 3 times s.d) \n","#         outD = X['DemandUnits'].mean() + (3*X['DemandUnits'].std())\n","#         outO = X['OrderedUnits'].mean() + (3*X['OrderedUnits'].std())\n","#         outC = X['CollectedUnits'].mean() + (3*X['CollectedUnits'].std())\n","\n","#     ### dropping rows of data where Demand, Ordered or Collected grater than outlier\n","#         X.drop((X[(X['DemandUnits'] >= outD) | \n","#                   (X['OrderedUnits'] >= outO) | \n","#                   (X['CollectedUnits'] >= outC) |\n","#                   (X['SimpleOrderPropensity'] > 1) | \n","#                   (X['CollectOrderPropensity'] > 1)]).index, 0)\n","        \n","#     ### removing extra or unnecessary columns to reduce model complexity (if True)\n","#         if self.removExtraCols:\n","#             X.drop(X[[\"Date\", \"ProductKey\", \"LocationKey\", \n","#                       \"DemandUnits\", \"OrderedUnits\", \"CollectedUnits\", \n","#                       \"IsBankHoliday\", \"Seasonal\", \"IsHub\", \n","#                       \"Latitude\", \"Longitude\"]], 1)\n","#         else:\n","#             X.drop(X[[\"Date\", \"ProductKey\", \"LocationKey\", \n","#                       \"DemandUnits\", \"OrderedUnits\", \"CollectedUnits\"]], 1)\n","        \n","#     ### merging categorical columns to reduce datasize (and complexity)   \n","#         if self.mergCols:\n","#             X['prodHierarchy'] = X[\"HierarchyLevel2\"].astype(str) + \"-\" + X[\"HierarchyLevel1\"].astype(str)\n","#             X['locType'] = X[\"LocationType2\"].astype(str) + \"-\" + X[\"LocationType1\"].astype(str)\n","#             X.drop([\"HierarchyLevel1\", \"HierarchyLevel2\", \"LocationType1\", \"LocationType2\"], 1)\n","        \n","#     ### changing dtypes for categorical variables to string from integer/boolean\n","#         if self.allCatStr:\n","            \n","            "],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Selecting random Sample of Data"],"metadata":{}},{"cell_type":"code","source":["lines = main_df.shape[0]"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["skiplines = np.random.choice(np.arange(1, lines), size=lines-1000000, replace=False)\n","\n","#sort the list\n","skiplines=np.sort(skiplines)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading only one million sample rows that are required"],"metadata":{}},{"cell_type":"code","source":["main_df = pd.read_csv(\"../input/main1m/main1m.csv\", skiprows=skiplines, nrows=1000000) #"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## String conversion of categorical variable for one hot encoding"],"metadata":{}},{"cell_type":"code","source":["main_df['YearWeek'] = main_df[\"YearWeek\"].astype(str)\n","main_df['DayOfWeek'] = main_df[\"DayOfWeek\"].astype(str)\n","main_df['IsBankHoliday'] = main_df[\"IsBankHoliday\"].astype(str)\n","main_df['IsWorkingDay'] = main_df[\"IsWorkingDay\"].astype(str)\n","main_df['HierarchyLevel1'] = main_df[\"HierarchyLevel1\"].astype(str)\n","main_df['HierarchyLevel2'] = main_df[\"HierarchyLevel2\"].astype(str)\n","main_df['Seasonal'] = main_df[\"Seasonal\"].astype(str)\n","main_df['IsHub'] = main_df[\"IsHub\"].astype(str)\n","main_df['LocationType1'] = main_df[\"LocationType1\"].astype(str)\n","main_df['LocationType2'] = main_df[\"LocationType2\"].astype(str)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Seperating feature and target variables"],"metadata":{}},{"cell_type":"code","source":["Xall = pd.DataFrame()\n","# Xless = pd.DataFrame()\n","Xall = main_df[[\"YearWeek\", \"DayOfWeek\", \"IsWorkingDay\", \"IsBankHoliday\", \n","                \"HierarchyLevel1\", \"HierarchyLevel2\", \"Seasonal\", \"IsHub\", \n","                \"LocationType1\", \"LocationType2\", \"Latitude\", \"Longitude\"]].copy()\n","\n","# Xless = main_df[[\"YearWeek\", \"DayOfWeek\", \"IsWorkingDay\"]].copy()\n","# Xless['prodHierarchy'] = main_df[\"HierarchyLevel2\"] + \"-\" + main_df[\"HierarchyLevel1\"]\n","# Xless['locType'] = main_df[\"LocationType2\"] + \"-\" + main_df[\"LocationType1\"]\n","\n","yS = main_df[\"SimpleOrderPropensity\"]\n","yC = main_df[\"CollectOrderPropensity\"]"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["del main_df\n","gc.collect()"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transforming feature variables "],"metadata":{}},{"cell_type":"code","source":["catFeat = [\"YearWeek\", \"DayOfWeek\", \"IsWorkingDay\", \"IsBankHoliday\", \"HierarchyLevel1\", \"HierarchyLevel2\", \n","           \"Seasonal\", \"IsHub\", \"LocationType1\", \"LocationType2\"]\n","\n","numFeat = [\"Latitude\", \"Longitude\"]\n","\n","numTransf = MinMaxScaler()\n","Xnum = numTransf.fit_transform(Xall[numFeat])\n","\n","Xcat = pd.get_dummies(Xall[catFeat], sparse = False)\n","\n","Xall = np.concatenate((Xnum, Xcat), axis=1)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train test split 80/20 ratio"],"metadata":{}},{"cell_type":"code","source":["X_train, X_test, yS_train, yS_test = train_test_split(Xall, yS, test_size = 0.2, random_state = 0)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Linear Regression Model"],"metadata":{}},{"cell_type":"code","source":["linReg = LinearRegression().fit(X_train, yS_train)\n","print(\"Training set score: {:.4f}\".format(linReg.score(X_train, yS_train)))\n","print(\"Test set score: {:.4f}\".format(linReg.score(X_test, yS_test)))"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = linReg.predict(X_test)\n","rmse = np.sqrt(mean_squared_error(yS_test, y_pred))\n","print(\"Root Mean Squared Error: {:.4f}\".format(rmse))\n","mae = mean_absolute_error(yS_test, y_pred)\n","print('Mean Absolute Error: ', mae)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ridge Regression Model"],"metadata":{}},{"cell_type":"code","source":["ridge = Ridge().fit(X_train, yS_train)\n","print(\"Training set score: {:.4f}\".format(ridge.score(X_train, yS_train)))\n","print(\"Test set score: {:.4f}\".format(ridge.score(X_test, yS_test)))\n","print(\"Number of features used: {}\".format(np.sum(ridge.coef_ != 0)))"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_score = 0\n","best_alpha = 0\n","for alpha in [0.01, 0.1, 1, 10, 100]:\n","    ridge = Ridge(alpha=alpha).fit(X_train, yS_train)\n","    testscore = ridge.score(X_test, yS_test)\n","    if testscore > best_score:\n","        best_score = testscore\n","        best_alpha = alpha\n","print('Best score: {:.4f}'.format(best_score))\n","print('Best alpha: {:.2f}'.format(best_alpha))"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ridge = Ridge(alpha=best_alpha).fit(X_train, yS_train)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = ridge.predict(X_test)\n","rmse = np.sqrt(mean_squared_error(yS_test, y_pred))\n","print(\"Root Mean Squared Error: {:.4f}\".format(rmse))\n","mae = mean_absolute_error(yS_test, y_pred)\n","print('Mean Absolute Error: ', mae)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LASSO Regression Model"],"metadata":{}},{"cell_type":"code","source":["lasso1 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, yS_train)\n","print(\"Training set score: {:.4f}\".format(lasso1.score(X_train, yS_train)))\n","print(\"Test set score: {:.4f}\".format(lasso1.score(X_test, yS_test)))\n","print(\"Number of features used: {}\".format(np.sum(lasso1.coef_ != 0)))"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lasso2 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, yS_train)\n","print(\"Training set score: {:.4f}\".format(lasso2.score(X_train, yS_train)))\n","print(\"Test set score: {:.4f}\".format(lasso2.score(X_test, yS_test)))\n","print(\"Number of features used: {}\".format(np.sum(lasso2.coef_ != 0)))"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = lasso2.predict(X_test)\n","rmse = np.sqrt(mean_squared_error(yS_test, y_pred))\n","print(\"Root Mean Squared Error: {:.4f}\".format(rmse))\n","mae = mean_absolute_error(yS_test, y_pred)\n","print(\"Mean Absolute Error: \", mae)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Random Forest Regression Model"],"metadata":{}},{"cell_type":"code","source":["forest = RandomForestRegressor(n_estimators=10, max_depth=10, bootstrap=True, n_jobs=-1)\n","forest.fit(X_train, yS_train)\n","print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, yS_train)))\n","print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, yS_test)))"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = forest.predict(X_test)\n","rmse = np.sqrt(mean_squared_error(yS_test, y_pred))\n","print(\"Root Mean Squared Error: {:.4f}\".format(rmse))\n","mae = mean_absolute_error(yS_test, y_pred)\n","print('Mean Absolute Error: ', mae)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["forest1 = RandomForestRegressor(n_estimators=5, max_depth=20, bootstrap=True, n_jobs=-1)\n","forest1.fit(X_train, yS_train)\n","print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, yS_train)))\n","print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, yS_test)))"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = forest1.predict(X_test)\n","rmse = np.sqrt(mean_squared_error(yS_test, y_pred))\n","print(\"Root Mean Squared Error: {:.4f}\".format(rmse))\n","mae = mean_absolute_error(yS_test, y_pred)\n","print('Mean Absolute Error: ', mae)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## XGBoost Regression Model"],"metadata":{}},{"cell_type":"code","source":["xgb_reg = xgb.XGBRegressor()\n","xgb_reg.fit(X_train, yS_train)\n","y_pred = xgb_reg.predict(X_test)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Accuracy on test set: {:.3f}\".format(xgb_reg.score(X_test, yS_test)))"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rmse = np.sqrt(mean_squared_error(yS_test, y_pred))\n","print(\"Root Mean Squared Error: {:.4f}\".format(rmse))\n","mae = mean_absolute_error(yS_test, y_pred)\n","print('Mean Absolute Error: ', mae)"],"metadata":{},"execution_count":null,"outputs":[]}]}